Device: cuda

Neural Network Architecture:
CNN_BiLSTM_CTC_V5(
  (cnn): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace=True)
    (20): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)
    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (23): ReLU(inplace=True)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)
  )
  (lstm): LSTM(1024, 256, num_layers=2, batch_first=True, bidirectional=True)
  (fc): Linear(in_features=512, out_features=80, bias=True)
)

Hyperparameters:
img_height: 32
num_channels: 1
n_classes: 80
n_h: 256
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
learning_rate: 0.001
criterion: CTCLoss()
num_epochs: 75
batch_size: 8
transform: Resize with aspect ratio and ToTensor
Starting training process: 

Epoch 1 completed. Training Loss: 2.0240, Validation Loss: 1.0705
Epoch 2 completed. Training Loss: 0.8887, Validation Loss: 0.5921
Epoch 3 completed. Training Loss: 0.6137, Validation Loss: 0.4219
Epoch 4 completed. Training Loss: 0.5014, Validation Loss: 0.4083
Epoch 5 completed. Training Loss: 0.4287, Validation Loss: 0.3704
Epoch 6 completed. Training Loss: 0.3713, Validation Loss: 0.3837
Epoch 7 completed. Training Loss: 0.3292, Validation Loss: 0.3087
Epoch 8 completed. Training Loss: 0.2963, Validation Loss: 0.3157
Epoch 9 completed. Training Loss: 0.2648, Validation Loss: 0.3117
Epoch 10 completed. Training Loss: 0.2418, Validation Loss: 0.2962
Epoch 11 completed. Training Loss: 0.2213, Validation Loss: 0.3515
Epoch 12 completed. Training Loss: 0.2065, Validation Loss: 0.3028
Epoch 13 completed. Training Loss: 0.1915, Validation Loss: 0.3578
Epoch 14 completed. Training Loss: 0.1810, Validation Loss: 0.3556
Epoch 15 completed. Training Loss: 0.1679, Validation Loss: 0.3471
Epoch 16 completed. Training Loss: 0.1597, Validation Loss: 0.3325
Epoch 17 completed. Training Loss: 0.1516, Validation Loss: 0.3666
Epoch 18 completed. Training Loss: 0.1434, Validation Loss: 0.3667
Epoch 19 completed. Training Loss: 0.1401, Validation Loss: 0.3887
Epoch 20 completed. Training Loss: 0.1332, Validation Loss: 0.3638
Epoch 21 completed. Training Loss: 0.1292, Validation Loss: 0.4010
Epoch 22 completed. Training Loss: 0.1252, Validation Loss: 0.3647
Epoch 23 completed. Training Loss: 0.1213, Validation Loss: 0.4079
Epoch 24 completed. Training Loss: 0.1225, Validation Loss: 0.3943
Epoch 25 completed. Training Loss: 0.1183, Validation Loss: 0.4162
Epoch 26 completed. Training Loss: 0.1166, Validation Loss: 0.4099
Epoch 27 completed. Training Loss: 0.1124, Validation Loss: 0.4023
Epoch 28 completed. Training Loss: 0.1118, Validation Loss: 0.4016
Epoch 29 completed. Training Loss: 0.1089, Validation Loss: 0.4212
Epoch 30 completed. Training Loss: 0.1071, Validation Loss: 0.4106
Epoch 31 completed. Training Loss: 0.1061, Validation Loss: 0.4324
Epoch 32 completed. Training Loss: 0.1046, Validation Loss: 0.4745
Epoch 33 completed. Training Loss: 0.1036, Validation Loss: 0.4539
Epoch 34 completed. Training Loss: 0.1018, Validation Loss: 0.4431
Epoch 35 completed. Training Loss: 0.0994, Validation Loss: 0.4661
Epoch 36 completed. Training Loss: 0.0951, Validation Loss: 0.4418
Epoch 37 completed. Training Loss: 0.0972, Validation Loss: 0.4284
Epoch 38 completed. Training Loss: 0.0972, Validation Loss: 0.4323
Epoch 39 completed. Training Loss: 0.0978, Validation Loss: 0.4372
Epoch 40 completed. Training Loss: 0.0997, Validation Loss: 0.4508
Epoch 41 completed. Training Loss: 0.0945, Validation Loss: 0.4891
Epoch 42 completed. Training Loss: 0.0939, Validation Loss: 0.4777
Epoch 43 completed. Training Loss: 0.0971, Validation Loss: 0.5018
Epoch 44 completed. Training Loss: 0.0958, Validation Loss: 0.4816
Epoch 45 completed. Training Loss: 0.0913, Validation Loss: 0.4802
Epoch 46 completed. Training Loss: 0.0899, Validation Loss: 0.4586
Epoch 47 completed. Training Loss: 0.0903, Validation Loss: 0.4686
Epoch 48 completed. Training Loss: 0.0946, Validation Loss: 0.4784
Epoch 49 completed. Training Loss: 0.0896, Validation Loss: 0.4594
Epoch 50 completed. Training Loss: 0.0907, Validation Loss: 0.5094
Epoch 51 completed. Training Loss: 0.0927, Validation Loss: 0.4434
Epoch 52 completed. Training Loss: 0.0942, Validation Loss: 0.4670
Epoch 53 completed. Training Loss: 0.0913, Validation Loss: 0.5322
Epoch 54 completed. Training Loss: 0.0910, Validation Loss: 0.5034
Epoch 55 completed. Training Loss: 0.0917, Validation Loss: 0.5026
Epoch 56 completed. Training Loss: 0.0913, Validation Loss: 0.5429
Epoch 57 completed. Training Loss: 0.0884, Validation Loss: 0.5135
Epoch 58 completed. Training Loss: 0.0983, Validation Loss: 0.4883
Epoch 59 completed. Training Loss: 0.0953, Validation Loss: 0.4739
Epoch 60 completed. Training Loss: 0.0941, Validation Loss: 0.4630
Epoch 61 completed. Training Loss: 0.0938, Validation Loss: 0.4619
Epoch 62 completed. Training Loss: 0.0926, Validation Loss: 0.4575
Epoch 63 completed. Training Loss: 0.0966, Validation Loss: 0.4318
Epoch 64 completed. Training Loss: 0.0999, Validation Loss: 0.4450
Epoch 65 completed. Training Loss: 0.0959, Validation Loss: 0.4857
Epoch 66 completed. Training Loss: 0.0984, Validation Loss: 0.4515
Epoch 67 completed. Training Loss: 0.0990, Validation Loss: 0.4888
Epoch 68 completed. Training Loss: 0.0964, Validation Loss: 0.4832
Epoch 69 completed. Training Loss: 0.0968, Validation Loss: 0.4810
Epoch 70 completed. Training Loss: 0.1017, Validation Loss: 0.4721
Epoch 71 completed. Training Loss: 0.1040, Validation Loss: 0.5105
Epoch 72 completed. Training Loss: 0.1050, Validation Loss: 0.5168
Epoch 73 completed. Training Loss: 0.1089, Validation Loss: 0.4773
Epoch 74 completed. Training Loss: 0.1068, Validation Loss: 0.4588
Epoch 75 completed. Training Loss: 0.1088, Validation Loss: 0.4708
Model saved as cnn_lstm_ctc_handwritten_v0_75ep_2-Layered-BiLSTM.pth
