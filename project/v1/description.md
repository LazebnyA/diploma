## Опис моделі

Це базова модель, яку можна в подальшому покращувати. Побудова цієї моделі не передбачала її використання, лише тестування працездатності коду - завантаження набору даних, трансформацій зображень та правильності підібраних входів/виходів шарів нейронної мережі CRNN.

### Архітектура

```
CNN_LSTM_CTC_V0(
  (cnn): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (lstm): LSTM(1024, 256, batch_first=True)
  (fc): Linear(in_features=256, out_features=80, bias=True)
)
```

Як можна побачити CNN є базовою, вона не складається з кількох блоків. Окрім цього не використовується `batch_normalization`. 

Оскільки, згорткова нейронна мережа достатньо проста, з її допомогою не вдається вилучити достатньо ознак (або достатньо потрібних ознак). Через це LSTM отримує на вході для кожного часового кроку недостатньо інформації для коректного навчання (запам'ятовування патернів в даних, оскільки ці патерни були вилучені в недостатньому обсязі).

### Тренувальні дані
- Модель була навчена на вибірці зі всіх даних `iam_words`. Без розбиття на тренувальну, тестову та валідаційну вибірки. 
- В результаті, моделі не вдалося показати падіння середнього значення *Loss-функції* на епоху нижче позначки в 0.4860, навіть на тренувальній вибірці після тривалого навчання. 

### Чому обрані такі гіперпараметри? 
- Описано в `notes.md`

### Порівняння з іншими моделями
За сукупністю факторів дана модель не буде розглядатися для порівняння з іншими моделями. Дана модель була створена як найперша модель, для відлагодження коду та з навчальних цілей - зрозуміти як передаються вилучені за допомогою CNN ознаки до LSTM, як налаштувати пряме проходження моделі з CTC та як навчати модель за допомогою *CTCLoss* як *Loss-функцією*.