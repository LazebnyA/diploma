В даній папці була побудована базова модель CNN + RNN (LSTM) + CTC для розпізнавання рукописних слів. Оскільки модель - базова, обрана архітектура не була підібрана найкращим чином, але загальна логіка підбору гіперпараметрів залишається такою:

## Чому такі гіперпараметри?
___
#### Висота зображення (img_height = 32)

- 32 — стандартна висота в OCR-моделях. Вона достатньо мала для швидкої обробки, але містить достатньо інформації для розпізнавання букв.

#### Кількість каналів (num_channels = 1)

- Чорно-білі зображення (глибина 1 канал) зменшують обчислювальні витрати, оскільки колір тут не несе корисної інформації.

#### Кількість класів (n_classes = len(vocab) + 1)

- Кількість символів у словнику + 1 для CTC blank (порожнього символу).

#### Кількість нейронів у LSTM (n_h = 256)

- 256 — баланс між складністю та швидкістю навчання. Дозволяє запам’ятовувати довгі залежності між символами.

#### Оптимізатор (Adam, lr = 0.001)

- Adam добре працює з послідовними даними, швидко адаптується до особливостей навчання.
Learning rate = 0.001 вибраний для стабільної і швидкої збіжності без ризику занадто великого розкиду ваг.

#### Кількість епох (num_epochs = 50)

- 50 епох достатньо для гарного навчання без перенавчання (перевіряється на валідації).

#### Розмір батчу (batch_size = 8)

- Баланс між використанням пам’яті та стабільністю градієнта при оновленні ваг.
___

Архітектура CNN для вилучення ознак на даному етапі є максимально поверхневою, тому можна очікувати, що вилучені ознаки будуть не до кінця відображати всі можливі паттерни.

Щодо архітектури LSTM, вона також є поверхневою, її можна покращувати.

## Шляхи для покращення
### Покращення рекурентної частини (LSTM)
___
#### BiLSTM замість LSTM

- Проблема: Звичайна LSTM враховує лише попередній контекст, а розпізнавання літер може залежати від наступних символів.
- Рішення: BiLSTM (двонаправлена LSTM) аналізує контекст в обох напрямках (зліва направо і справа наліво), що покращує якість розпізнавання складних символів та скорочень.
- Очікуваний ефект: Точність розпізнавання покращиться, особливо для рукописного тексту з неоднорідними літерами.

#### Збільшення кількості шарів LSTM

- Додавання ще одного шару LSTM або BiLSTM допоможе краще враховувати складні залежності між символами.
- Але збільшення шарів може зробити навчання повільнішим, тому треба слідкувати за балансом.
___
### Покращення згорткової частини (CNN)

- #### Додавання більшої кількості CNN-шарів
    - Якщо збільшити глибину CNN, то модель зможе краще витягувати особливості символів, враховуючи складні деталі написання.
    - Наприклад, ResNet або EfficientNet замість стандартних CNN може покращити якість витягнення ознак.

- #### Збільшення розміру фільтрів CNN
  - Якщо використовуються малі фільтри (наприклад, 3x3), можна спробувати більші (5x5 або 7x7) для захоплення глобальних особливостей літер.

___

### Покращення навчання

- #### Data Augmentation (Аугментація даних)
  - Розширення та нахили символів — щоб навчити модель працювати з різними стилями написання.
  - Додавання шуму — допоможе моделі бути стійкою до реальних умов (наприклад, поганої якості сканів).
  - Зміна контрасту та яскравості — допоможе враховувати варіації в освітленні.

- #### Заморожування CNN на початку навчання
  - Якщо використовувати готову предтреновану CNN (наприклад, ResNet), можна на початку заморозити її шари, щоб навчання йшло швидше і стабільніше.

___

## Подальші покращення:

- ### Модифікація CNN для кращого вилучення ознак.
- ### Модифікація LSTM до двошарової BiLSTM 
  - (в 2-ій версії, після оцінки наскільки вплинуло вдосконалення CNN)
