### Інше:

- [x] Розробити порівняння кількості фільтрів і n_h одночасно, оскільки чим більше фільтрів - тим більше ознак вилучається, і тим більше ознак потрібно передавати між часовими кроками в h_t

- [ ] Переробити схеми, щоб було видно, що кожен часовий крок LSTM відповідає за передбачення свого символа.

- [ ] Починаючи з розділу про Приховані Марківські Моделі. Розписати теорію про HMM більш детально з посиланням на джерело. Окрім того вказати про гібридні моделі, що поєднують нейронні мережі та HMM. 

- [ ] Розписати краще розділ про CRNN. Приклади на фундаментальні реалізації і на греків. Менш детально розписувати про Gated модель. 
 
- [ ] Відформатувати розділ про Трансформери, та увагу. Додати посилання на першоджерело.

- [ ] Розробити і заповнити табличку порівняння методів. 

___

### Базова модель

- [x] Переробити саму початкову модель - взяти як початкову - модель з maxpool (2, 2) на другому шарі CNN. 
- [ ] Зробити порівняння для різних гіперпараметрів уже з обраною версією.
  - [x] Порівняння для оптимізаторів та lr
  - [x] Порівняння для розмірів батчів
    - Було обрано 8, оскільки показав найкращі результати  по метрикам + швидкодії
  - [x] Порівняти окремо для кількості фільтрів та для кількості n_h
  - [x] Порівняти разом для кількості фільтрів та для кількості n_h
  - [x] Порівняти для кількості шарів LSTM
  - [ ] Натренувати модель для оптимальної епохи - де лосс на валідаційному наборі найменший і далі лосс збільшується.
  - [ ] Затестити перформанс моделі на тестовому наборі

- [ ] Поглибити модель, проаналізувати вплив додавання технік регуляризації - нормалізації батчів, дропауту.   

___

### Глибша CNN - VGG16 подібна архітектура (якщо буде час (в останню чергу))

- [ ] Збільшити глибину, порівняти з базовою моделлю.
- [ ] Зробити порівняння для різних гіперпараметрів уже з обраною версією.
  - [ ] Порівняння для оптимізаторів та lr
  - [ ] Порівняння для розмірів батчів
  - [ ] Порівняння для кількості фільтрів
  - [ ] Порівняти для параметрів n_h та кількості шарів LSTM
    - Як `n_h` було обрано 512 через найкращу стабільність навчання, та значення метрик на вал. вибірці. При тому, що середній час навчання за епоху займав такий самий час як і для інших
  - [ ] Порівняти для 1-шарової та 2-шарової BiLSTM
  - [ ] Натренувати модель для оптимальної епохи - де лосс на валідаційному наборі найменший і далі лосс збільшується.
  - [ ] Затестити перформанс моделі на тестовому наборі
  
___

### ResNet18 подібна архітектура

- [x] Натренувати на базових гіперпараметрах
- [ ] Зробити порівняння для різних гіперпараметрів уже з обраною версією.
  - [x] Порівняння для оптимізаторів та lr
    - Обрано RMSprop з learning rate = 0.0001
  - [x] Порівняння для розмірів батчів
    - Обрано розмір = 16, оскільки забезпечує точність + швидкодію
  - [x] Порівняння для кількості фільтрів
    - Найкраще себе показав результат з кількістю фільтрів 48, що відповідає 48-96-192-274
  - [x] Порівняти для параметрів n_h та кількості шарів LSTM
    - Було вирішено зупинитися на n_h = 1024, та кількості фільтрів 64. Оскільки така комбінація забезпечує найвищу точність та швидкість навчання (по епохах, але не по часу).
  - [ ] Натренувати модель для оптимальної епохи - де лосс на валідаційному наборі найменший і далі лосс збільшується.
  - [ ] Затестити перформанс моделі на тестовому наборі

___

### Data Preprocessing

- [ ] Для моделей - порівнюємо техніки препроцесінгу, спочатку ходу навчання моделей з техніками, потім на самописному наборі.
- [ ] Після цього порівнюємо те ж саме з аугментацією даних.

___

### Data Postprocessing (Опціонально)

- [ ] Для моделей - порівнюємо техніки постпроцесінгу

___