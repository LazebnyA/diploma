Device: cuda
Loaded initial random weights from cnn_lstm_ctc_handwritten_v1_initial_imH64.pth

Neural Network Architecture:
CNN_LSTM_CTC_V2_CNN_more_filters_batch_norm_deeper_vgg16like(
  (cnn): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01, inplace=True)
    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.01, inplace=True)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.01, inplace=True)
    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.01, inplace=True)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): LeakyReLU(negative_slope=0.01, inplace=True)
    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): LeakyReLU(negative_slope=0.01, inplace=True)
    (20): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)
    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (23): LeakyReLU(negative_slope=0.01, inplace=True)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): LeakyReLU(negative_slope=0.01, inplace=True)
    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): LeakyReLU(negative_slope=0.01, inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): LeakyReLU(negative_slope=0.01, inplace=True)
    (33): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)
  )
  (lstm): LSTM(2048, 256, batch_first=True, bidirectional=True)
  (fc): Linear(in_features=512, out_features=80, bias=True)
)

Hyperparameters:
img_height: 64
num_channels: 1
n_classes: 80
n_h: 256
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
learning_rate: 0.001
criterion: CTCLoss()
num_epochs: 30
batch_size: 8
transform: Resize with aspect ratio. Contrast/Brightness Transform, Otsu-binarization
dataset: IAM Lines Dataset (writer-independent split)
Starting training process: 


Epoch 1 Results:
Training - Loss: 1.9021, CER: 0.5767, WER: 0.7723
Validation - Loss: 1.0667, CER: 0.3069, WER: 0.5892

Epoch 2 Results:
Training - Loss: 0.9060, CER: 0.2666, WER: 0.5459
Validation - Loss: 16.4045, CER: 0.9948, WER: 0.9877

Epoch 3 Results:
Training - Loss: 0.6888, CER: 0.2036, WER: 0.4579
Validation - Loss: 0.6762, CER: 0.1873, WER: 0.4299

Epoch 4 Results:
Training - Loss: 0.5689, CER: 0.1694, WER: 0.4040
Validation - Loss: 0.6168, CER: 0.1751, WER: 0.4066

Epoch 5 Results:
Training - Loss: 0.4823, CER: 0.1449, WER: 0.3603
Validation - Loss: 0.5786, CER: 0.1575, WER: 0.3788

Epoch 6 Results:
Training - Loss: 0.4277, CER: 0.1295, WER: 0.3299
Validation - Loss: 29.8129, CER: 1.5249, WER: 0.9972

Epoch 7 Results:
Training - Loss: 0.3821, CER: 0.1171, WER: 0.3085
Validation - Loss: 0.5816, CER: 0.1577, WER: 0.3742

Epoch 8 Results:
Training - Loss: 0.3427, CER: 0.1079, WER: 0.2907
Validation - Loss: 0.5229, CER: 0.1372, WER: 0.3420

Epoch 9 Results:
Training - Loss: 0.3161, CER: 0.1003, WER: 0.2741
Validation - Loss: 1.4407, CER: 0.2713, WER: 0.6221

Epoch 10 Results:
Training - Loss: 0.2857, CER: 0.0932, WER: 0.2599
Validation - Loss: 0.5232, CER: 0.1340, WER: 0.3348

Epoch 11 Results:
Training - Loss: 0.2600, CER: 0.0869, WER: 0.2470
Validation - Loss: 0.6121, CER: 0.1328, WER: 0.3327

Epoch 12 Results:
Training - Loss: 0.2345, CER: 0.0799, WER: 0.2318
Validation - Loss: 0.5145, CER: 0.1235, WER: 0.3157

Epoch 13 Results:
Training - Loss: 0.2112, CER: 0.0749, WER: 0.2208
Validation - Loss: 0.5331, CER: 0.1265, WER: 0.3219

Epoch 14 Results:
Training - Loss: 0.1939, CER: 0.0708, WER: 0.2126
Validation - Loss: 0.5725, CER: 0.1351, WER: 0.3350

Epoch 15 Results:
Training - Loss: 0.1747, CER: 0.0654, WER: 0.2003
Validation - Loss: 0.5666, CER: 0.1254, WER: 0.3190

Epoch 16 Results:
Training - Loss: 0.1633, CER: 0.0625, WER: 0.1934
Validation - Loss: 0.6088, CER: 0.1305, WER: 0.3295

Epoch 17 Results:
Training - Loss: 0.1482, CER: 0.0594, WER: 0.1860
Validation - Loss: 0.6028, CER: 0.1273, WER: 0.3234

Epoch 18 Results:
Training - Loss: 0.1392, CER: 0.0573, WER: 0.1814
Validation - Loss: 0.6282, CER: 0.1293, WER: 0.3269

Epoch 19 Results:
Training - Loss: 0.1286, CER: 0.0545, WER: 0.1754
Validation - Loss: 0.9380, CER: 0.1664, WER: 0.4058

Epoch 20 Results:
Training - Loss: 0.1203, CER: 0.0526, WER: 0.1696
Validation - Loss: 0.6528, CER: 0.1263, WER: 0.3229

Epoch 21 Results:
Training - Loss: 0.1123, CER: 0.0508, WER: 0.1654
Validation - Loss: 0.6989, CER: 0.1284, WER: 0.3263

Epoch 22 Results:
Training - Loss: 0.1097, CER: 0.0495, WER: 0.1631
Validation - Loss: 0.6924, CER: 0.1286, WER: 0.3255

Epoch 23 Results:
Training - Loss: 0.1009, CER: 0.0476, WER: 0.1575
Validation - Loss: 0.7089, CER: 0.1295, WER: 0.3307

Epoch 24 Results:
Training - Loss: 0.0962, CER: 0.0467, WER: 0.1559
Validation - Loss: 0.7192, CER: 0.1268, WER: 0.3245

Epoch 25 Results:
Training - Loss: 0.0915, CER: 0.0456, WER: 0.1531
Validation - Loss: 0.7559, CER: 0.1292, WER: 0.3278

Epoch 26 Results:
Training - Loss: 0.0894, CER: 0.0449, WER: 0.1514
Validation - Loss: 0.7486, CER: 0.1320, WER: 0.3347

Epoch 27 Results:
Training - Loss: 0.0855, CER: 0.0439, WER: 0.1493
Validation - Loss: 1.8900, CER: 0.1778, WER: 0.4280

Epoch 28 Results:
Training - Loss: 0.0832, CER: 0.0435, WER: 0.1481
Validation - Loss: 0.7509, CER: 0.1309, WER: 0.3322

Epoch 29 Results:
Training - Loss: 0.0819, CER: 0.0428, WER: 0.1464
Validation - Loss: 0.8090, CER: 0.1304, WER: 0.3302

Epoch 30 Results:
Training - Loss: 0.0748, CER: 0.0411, WER: 0.1417
Validation - Loss: 0.8351, CER: 0.1274, WER: 0.3212
Model saved as cnn_lstm_ctc_handwritten_v0_word_30ep_CNN-BiLSTM-CTC_CNN-VGG16_BiLSTM-1dim.pth
Time elapsed: 49504.64072275162
Start time: 1744997817.0161889
End time: 1745047321.6569116
